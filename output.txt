diff --git a/output.txt b/output.txt
index b4813d0..e69de29 100644
--- a/output.txt
+++ b/output.txt
@@ -1,1196 +0,0 @@
-diff --git a/output.txt b/output.txt
-index fd6678c..e69de29 100644
---- a/output.txt
-+++ b/output.txt
-@@ -1,408 +0,0 @@
--diff --git a/src/features/chat/components/empty-chat-state.tsx b/src/features/chat/components/empty-chat-state.tsx
--index 74e82ab..8676bc9 100644
----- a/src/features/chat/components/empty-chat-state.tsx
--+++ b/src/features/chat/components/empty-chat-state.tsx
--@@ -24,7 +24,7 @@ export const EmptyChatState: React.FC<EmptyChatStateProps> = ({
--         <div className="mb-4">
--           <MessageSquare className="h-8 w-8 text-muted-foreground" />
--         </div>
---        <h3 className="text-lg font-medium">Welcome to Chat</h3>
--+        <h3 className="text-lg font-medium">Welcome to Cond8 Chat</h3>
--         <p className="text-sm text-muted-foreground mb-6">Start a new conversation or use the shortcuts below</p>
--       </div>
--       <ShortcutsCheatsheet shortcuts={SHORTCUTS} />
--diff --git a/src/features/chat/services/ollama-wrapper.ts b/src/features/chat/services/ollama-wrapper.ts
--deleted file mode 100644
--index 749142b..0000000
----- a/src/features/chat/services/ollama-wrapper.ts
--+++ /dev/null
--@@ -1,84 +0,0 @@
---import { DEFAULT_CONFIG, OllamaConfig } from '../types/ollama-config';
---import { OllamaChatRequest } from '../types/ollama-request';
---import { OllamaStreamResponse, StreamCallback } from '../types/ollama-response';
---
---// Types
---export interface OllamaTool {
---  type: 'function';
---  function: {
---    name: string;
---    description: string;
---    parameters: {
---      type: 'object';
---      properties: Record<
---        string,
---        {
---          type: string;
---          description?: string;
---          enum?: string[];
---        }
---      >;
---      required?: string[];
---    };
---  };
---}
---
---// Utility Functions
---export function createOllamaConfig(config: OllamaConfig = {}): Required<OllamaConfig> {
---  return {
---    ...DEFAULT_CONFIG,
---    ...config,
---  };
---}
---
---export async function chatWithTools(
---  request: OllamaChatRequest & { tools: OllamaTool[] },
---  onMessage: StreamCallback,
---  config: OllamaConfig = {},
---): Promise<void> {
---  const { baseUrl, defaultModel } = createOllamaConfig(config);
---  const url = new URL(`${baseUrl}/api/chat`);
---  const eventSource = new EventSource(url.toString(), {
---    withCredentials: false,
---  });
---
---  return new Promise((resolve, reject) => {
---    eventSource.onmessage = event => {
---      try {
---        const response = JSON.parse(event.data) as OllamaStreamResponse;
---        onMessage(response);
---
---        if (response.done) {
---          eventSource.close();
---          resolve();
---        }
---      } catch (error) {
---        eventSource.close();
---        reject(error);
---      }
---    };
---
---    eventSource.onerror = error => {
---      eventSource.close();
---      reject(error);
---    };
---
---    fetch(url.toString(), {
---      method: 'POST',
---      headers: {
---        'Content-Type': 'application/json',
---      },
---      body: JSON.stringify({
---        model: request.model || defaultModel,
---        messages: request.messages,
---        stream: true,
---        tools: request.tools,
---        tool_choice: request.tool_choice || 'auto',
---        options: request.options,
---      }),
---    }).catch(error => {
---      eventSource.close();
---      reject(error);
---    });
---  });
---}
--diff --git a/src/features/chat/store/chat-store.ts b/src/features/chat/store/chat-store.ts
--index 817a27f..7ba9ad9 100644
----- a/src/features/chat/store/chat-store.ts
--+++ b/src/features/chat/store/chat-store.ts
--@@ -26,7 +26,9 @@ interface ChatState {
--   deleteThread: (threadId: string) => void;
--   setCurrentThread: (threadId: string) => void;
--   addMessage: (message: Omit<Message, 'id' | 'timestamp'>) => void;
--+  updateLastMessage: (content: string) => void;
--   updateThreadTitle: (threadId: string, title: string) => void;
--+  setIsStreaming: (isStreaming: boolean) => void;
--   stopStreaming: () => void;
--   getRecentThreads: (limit?: number) => Thread[];
--   getTimeAgo: (timestamp: number) => string;
--@@ -127,6 +129,24 @@ export const useChatStore = create<ChatState>()(
--         });
--       },
-- 
--+      updateLastMessage: content => {
--+        set(state => {
--+          const thread = state.threads.find(t => t.id === state.currentThreadId);
--+          if (thread) {
--+            const lastMessage = thread.messages[thread.messages.length - 1];
--+            if (lastMessage && lastMessage.role === 'assistant') {
--+              // Create a new message object instead of mutating
--+              const updatedMessage = {
--+                ...lastMessage,
--+                content,
--+              };
--+              // Create a new messages array with the updated message
--+              thread.messages = [...thread.messages.slice(0, -1), updatedMessage];
--+            }
--+          }
--+        });
--+      },
--+
--       updateThreadTitle: (threadId, title) => {
--         set(state => {
--           const thread = state.threads.find(t => t.id === threadId);
--@@ -136,6 +156,12 @@ export const useChatStore = create<ChatState>()(
--         });
--       },
-- 
--+      setIsStreaming: isStreaming => {
--+        set(state => {
--+          state.isStreaming = isStreaming;
--+        });
--+      },
--+
--       stopStreaming: () => {
--         set(state => {
--           state.isStreaming = false;
--diff --git a/src/features/chat/store/ollama-store.ts b/src/features/chat/store/ollama-store.ts
--index 3f8f496..3a1fc38 100644
----- a/src/features/chat/store/ollama-store.ts
--+++ b/src/features/chat/store/ollama-store.ts
--@@ -1,3 +1,4 @@
--+import { OllamaService } from '@/lib/ollama';
-- import { create } from 'zustand';
-- import { persist } from 'zustand/middleware';
-- import { immer } from 'zustand/middleware/immer';
--@@ -8,6 +9,7 @@ interface OllamaState {
--   isLoading: boolean;
--   error: string | null;
--   lastFetched: number | null;
--+  ollamaService: OllamaService;
-- 
--   setUrl: (url: string) => void;
--   checkConnection: () => Promise<void>;
--@@ -24,32 +26,33 @@ export const useOllamaStore = create<OllamaState>()(
--       isLoading: false,
--       error: null,
--       lastFetched: null,
--+      ollamaService: new OllamaService(),
-- 
--       setUrl: url => {
--         set(state => {
--           state.ollamaUrl = url;
--+          state.ollamaService.updateConfig({ baseUrl: url });
--         });
--         get().checkConnection();
--       },
-- 
--       checkConnection: async () => {
--+        set(state => {
--+          state.isLoading = true;
--+          state.error = null;
--+        });
--+
--         try {
---          const res = await fetch(`${get().ollamaUrl}/api/chat`, {
---            method: 'POST',
---            headers: { 'Content-Type': 'application/json' },
---            body: JSON.stringify({
---              model: 'llama2',
---              messages: [{ role: 'user', content: 'test' }],
---              stream: false,
---            }),
---          });
---          if (!res.ok) throw new Error('Failed to connect to Ollama');
--+          const isConnected = await get().ollamaService.checkConnection();
--+
--           set(state => {
---            state.error = null;
--+            state.error = isConnected ? null : 'Failed to connect to Ollama';
--+            state.isLoading = false;
--           });
--         } catch (err) {
--           set(state => {
--             state.error = err instanceof Error ? err.message : 'Unknown error';
--+            state.isLoading = false;
--           });
--         }
--       },
--@@ -58,12 +61,7 @@ export const useOllamaStore = create<OllamaState>()(
--         const now = Date.now();
--         const state = get();
-- 
---        if (
---          !force &&
---          state.models &&
---          state.lastFetched &&
---          now - state.lastFetched < CACHE_MS
---        ) {
--+        if (!force && state.models && state.lastFetched && now - state.lastFetched < CACHE_MS) {
--           return state.models;
--         }
-- 
--@@ -73,10 +71,7 @@ export const useOllamaStore = create<OllamaState>()(
--         });
-- 
--         try {
---          const res = await fetch(`${state.ollamaUrl}/api/tags`);
---          if (!res.ok) throw new Error('Failed to fetch models');
---          const json = await res.json();
---          const models = json.models.map((m: { name: string }) => m.name);
--+          const models = await state.ollamaService.listModels();
-- 
--           set(s => {
--             s.models = models;
--@@ -97,6 +92,6 @@ export const useOllamaStore = create<OllamaState>()(
--     {
--       name: 'ollama-store',
--       partialize: state => ({ ollamaUrl: state.ollamaUrl }),
---    }
---  )
--+    },
--+  ),
-- );
--diff --git a/src/features/chat/tools/domain-tool-generator.ts b/src/features/chat/tools/domain-tool-generator.ts
--index 903586b..88b3c7b 100644
----- a/src/features/chat/tools/domain-tool-generator.ts
--+++ b/src/features/chat/tools/domain-tool-generator.ts
--@@ -1,5 +1,5 @@
-- import { z } from 'zod';
---import { zodToOllamaTool } from '../services/ollama-wrapper';
--+import { zodToOllamaTool } from '../utils/zod-to-ollama-tool';
-- import { Domain, domainClasses, Interface, Step } from './problem-solver';
-- 
-- // Schema for tool generation
--diff --git a/src/features/chat/tools/problem-solver.ts b/src/features/chat/tools/problem-solver.ts
--index d0a1bf4..126c437 100644
----- a/src/features/chat/tools/problem-solver.ts
--+++ b/src/features/chat/tools/problem-solver.ts
--@@ -1,6 +1,7 @@
--+import { OllamaChatResponse, OllamaService } from '@/lib/ollama';
-- import { z } from 'zod';
---import { OllamaChatResponse, OllamaService, zodToOllamaTool } from '../services/ollama-wrapper';
-- import { SYSTEM_PROMPT } from '../services/system-prompt';
--+import { zodToOllamaTool } from '../utils/zod-to-ollama-tool';
-- 
-- // Domain-specific class names with descriptions
-- const domainClasses = {
--diff --git a/src/features/chat/types/ollama-config.ts b/src/features/chat/types/ollama-config.ts
--deleted file mode 100644
--index c8802e6..0000000
----- a/src/features/chat/types/ollama-config.ts
--+++ /dev/null
--@@ -1,9 +0,0 @@
---export interface OllamaConfig {
---  baseUrl?: string;
---  defaultModel?: string;
---}
---
---export const DEFAULT_CONFIG: Required<OllamaConfig> = {
---  baseUrl: 'http://localhost:11434',
---  defaultModel: 'phi4-mini:latest',
---};
--diff --git a/src/features/chat/types/ollama-message.ts b/src/features/chat/types/ollama-message.ts
--deleted file mode 100644
--index e40a8a8..0000000
----- a/src/features/chat/types/ollama-message.ts
--+++ /dev/null
--@@ -1,14 +0,0 @@
---export interface OllamaMessage {
---  role: 'user' | 'assistant' | 'system';
---  content: string;
---  tool_calls?: OllamaToolCall[];
---}
---
---export interface OllamaToolCall {
---  id: string;
---  type: 'function';
---  function: {
---    name: string;
---    arguments: string;
---  };
---} 
--\ No newline at end of file
--diff --git a/src/features/chat/types/ollama-request.ts b/src/features/chat/types/ollama-request.ts
--deleted file mode 100644
--index 2630952..0000000
----- a/src/features/chat/types/ollama-request.ts
--+++ /dev/null
--@@ -1,17 +0,0 @@
---import { OllamaMessage } from './ollama-message';
---import { OllamaTool } from './ollama-tool';
---
---export interface OllamaChatRequest {
---  model: string;
---  messages: OllamaMessage[];
---  stream?: boolean;
---  tools?: OllamaTool[];
---  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };
---  options?: {
---    temperature?: number;
---    top_p?: number;
---    top_k?: number;
---    num_predict?: number;
---    stop?: string[];
---  };
---}
--diff --git a/src/features/chat/types/ollama-response.ts b/src/features/chat/types/ollama-response.ts
--deleted file mode 100644
--index f65cb2a..0000000
----- a/src/features/chat/types/ollama-response.ts
--+++ /dev/null
--@@ -1,29 +0,0 @@
---import { OllamaMessage } from './ollama-message';
---
---export interface OllamaChatResponse {
---  model: string;
---  created_at: string;
---  message: OllamaMessage;
---  done: boolean;
---  total_duration?: number;
---  load_duration?: number;
---  prompt_eval_duration?: number;
---  eval_duration?: number;
---}
---
---export interface OllamaStreamResponse {
---  model: string;
---  created_at: string;
---  message: OllamaMessage;
---  done: boolean;
---  total_duration?: number;
---  load_duration?: number;
---  prompt_eval_duration?: number;
---  eval_duration?: number;
---}
---
---export interface OllamaError {
---  error: string;
---}
---
---export type StreamCallback = (response: OllamaStreamResponse) => void; 
--\ No newline at end of file
--diff --git a/src/features/chat/types/ollama-tool.ts b/src/features/chat/types/ollama-tool.ts
--deleted file mode 100644
--index 96d07a6..0000000
----- a/src/features/chat/types/ollama-tool.ts
--+++ /dev/null
--@@ -1,19 +0,0 @@
---export interface OllamaTool {
---  type: 'function';
---  function: {
---    name: string;
---    description: string;
---    parameters: {
---      type: 'object';
---      properties: Record<
---        string,
---        {
---          type: string;
---          description?: string;
---          enum?: string[];
---        }
---      >;
---      required?: string[];
---    };
---  };
---} 
--\ No newline at end of file
--diff --git a/src/features/chat/utils/zod-to-ollama-tool.ts b/src/features/chat/utils/zod-to-ollama-tool.ts
--index dfee448..0a390c6 100644
----- a/src/features/chat/utils/zod-to-ollama-tool.ts
--+++ b/src/features/chat/utils/zod-to-ollama-tool.ts
--@@ -1,5 +1,5 @@
--+import { OllamaTool } from '@/lib/ollama';
-- import { z } from 'zod';
---import { OllamaTool } from '../services/ollama-wrapper';
-- 
-- export function zodToOllamaTool(
--   schema: z.ZodType<any>,
-diff --git a/src/features/chat/components/_chat-runner.tsx b/src/features/chat/components/_chat-runner.tsx
-index 70a0c28..4bd6ae6 100644
---- a/src/features/chat/components/_chat-runner.tsx
-+++ b/src/features/chat/components/_chat-runner.tsx
-@@ -1,114 +1,14 @@
- // src/features/chat/components/_chat-runner.tsx
--import { useEffect, useRef } from 'react';
-+import { useEffect } from 'react';
- import { useChatStore } from '../store/chat-store';
--import { useOllamaStore } from '../store/ollama-store';
--import { useAssistantConfigStore } from '../store/assistant-config-store';
--import { allTools } from '../tools';
-+import { runAssistantStream } from '../lib/stream-runner';
- 
- export function ChatRunner() {
--  const {
--    isStreaming,
--    setIsStreaming,
--    currentThreadId,
--    threads,
--    updateLastMessage,
--    addMessage,
--  } = useChatStore();
--  const { selectedModel, parameters } = useAssistantConfigStore();
--  const { client } = useOllamaStore();
--
--  const isRunningRef = useRef(false);
-+  const { isStreaming, currentThreadId } = useChatStore();
- 
-   useEffect(() => {
--    if (!isStreaming || isRunningRef.current || !currentThreadId) return;
--
--    const thread = threads.find(t => t.id === currentThreadId);
--    if (!thread) return;
--
--    const messages = thread.messages.map(m => ({
--      role: m.role,
--      content: m.content,
--    }));
--
--    let assistantContent = '';
--    isRunningRef.current = true;
--    addMessage({ role: 'assistant', content: '' });
--
--    client.chatWithTools(
--      {
--        model: selectedModel ?? 'default',
--        messages,
--        tools: allTools.map(t => t.tool),
--        tool_choice: 'auto',
--        options: parameters,
--      },
--      async (chunk) => {
--        // Handle streamed content
--        if ('content' in chunk && typeof chunk.content === 'string') {
--          assistantContent += chunk.content;
--          updateLastMessage(assistantContent);
--        }
--
--        // ðŸ§  Tool call support
--        if (chunk.message.tool_calls) {
--          for (const call of chunk.message.tool_calls) {
--            const tool = allTools.find(t => t.tool.function.name === call.function.name);
--            if (!tool) continue;
--
--            try {
--              const parsedArgs = JSON.parse(call.function.arguments || '{}');
--              const result = tool.parser.parse(parsedArgs); // validate with Zod
--
--              // Add the tool call to the thread
--              addMessage({
--                role: 'tool',
--                content: JSON.stringify(result, null, 2),
--              });
--
--              // Resume the conversation with tool result
--              const continuation = await client.chat({
--                model: selectedModel ?? 'default',
--                messages: [
--                  ...messages,
--                  {
--                    role: 'tool',
--                    name: call.function.name,
--                    content: JSON.stringify(result),
--                  },
--                ],
--                options: parameters,
--              });
--
--              if ('content' in continuation.message) {
--                addMessage({
--                  role: 'assistant',
--                  content: continuation.message.content,
--                });
--              }
--            } catch (err) {
--              console.error('Tool call error', err);
--              addMessage({
--                role: 'system',
--                content: `Tool execution failed: ${err instanceof Error ? err.message : 'Unknown error'}`,
--              });
--            }
--
--            setIsStreaming(false);
--            isRunningRef.current = false;
--            return;
--          }
--        }
--
--        if (chunk.done) {
--          setIsStreaming(false);
--          isRunningRef.current = false;
--        }
--      },
--    ).catch((err) => {
--      console.error('Streaming failed:', err);
--      setIsStreaming(false);
--      isRunningRef.current = false;
--    });
-+    if (!isStreaming || !currentThreadId) return;
-+    void runAssistantStream(currentThreadId);
-   }, [isStreaming, currentThreadId]);
- 
-   return null;
-diff --git a/src/features/chat/components/chat-box.tsx b/src/features/chat/components/chat-box.tsx
-deleted file mode 100644
-index a0c09f4..0000000
---- a/src/features/chat/components/chat-box.tsx
-+++ /dev/null
-@@ -1,85 +0,0 @@
--// src/features/chat/components/chat-box.tsx
--import React, { useState } from 'react';
--import { ChatService } from '../services/chat-service';
--import { useChatStore } from '../store/chat-store';
--import { useAssistantConfigStore } from '../store/assistant-config-store';
--import { useOllamaStore } from '../store/ollama-store';
--
--export function ChatBox() {
--  const [input, setInput] = useState('');
--  const { threads, currentThreadId, isStreaming } = useChatStore();
--  const { ollamaUrl } = useOllamaStore();
--  const { selectedModel } = useAssistantConfigStore();
--  
--  const chatService = ChatService.getInstance();
--  
--  const currentThread = threads.find(t => t.id === currentThreadId);
--  const messages = currentThread?.messages || [];
--
--  const handleSendMessage = async (e: React.FormEvent) => {
--    e.preventDefault();
--    
--    if (!input.trim() || isStreaming) return;
--    
--    const message = input.trim();
--    setInput('');
--    
--    await chatService.sendMessage(message);
--  };
--
--  const handleStopGeneration = () => {
--    chatService.stopStreaming();
--  };
--
--  return (
--    <div className="flex flex-col h-full">
--      <div className="p-2 text-sm text-gray-600 bg-gray-100 border-b">
--        Connected to: {ollamaUrl} | Model: {selectedModel || 'Default'}
--      </div>
--      
--      <div className="flex-1 overflow-auto p-4 space-y-4">
--        {messages.map(message => (
--          <div 
--            key={message.id} 
--            className={`${
--              message.role === 'user' ? 'bg-blue-100' : 'bg-gray-100'
--            } p-3 rounded-lg max-w-md ${
--              message.role === 'user' ? 'ml-auto' : 'mr-auto'
--            }`}
--          >
--            <div className="text-xs text-gray-500 mb-1">
--              {message.role === 'user' ? 'You' : 'Assistant'}
--            </div>
--            <div className="whitespace-pre-wrap">{message.content}</div>
--          </div>
--        ))}
--      </div>
--      
--      <form onSubmit={handleSendMessage} className="p-4 border-t flex">
--        <input
--          value={input}
--          onChange={e => setInput(e.target.value)}
--          placeholder="Type a message..."
--          className="flex-1 p-2 border rounded-l-md focus:outline-none focus:ring-2 focus:ring-blue-400"
--          disabled={isStreaming}
--        />
--        {isStreaming ? (
--          <button
--            type="button"
--            onClick={handleStopGeneration}
--            className="px-4 py-2 bg-red-500 text-white rounded-r-md hover:bg-red-600"
--          >
--            Stop
--          </button>
--        ) : (
--          <button
--            type="submit"
--            className="px-4 py-2 bg-blue-500 text-white rounded-r-md hover:bg-blue-600"
--          >
--            Send
--          </button>
--        )}
--      </form>
--    </div>
--  );
--} 
-\ No newline at end of file
-diff --git a/src/features/chat/services/chat-service.ts b/src/features/chat/services/chat-service.ts
-deleted file mode 100644
-index 3b37dc6..0000000
---- a/src/features/chat/services/chat-service.ts
-+++ /dev/null
-@@ -1,121 +0,0 @@
--// src/features/chat/services/chat-service.ts
--import { OllamaMessage, OllamaStreamResponse, OllamaTool } from '@/lib/ollama';
--import { useAssistantConfigStore } from '../store/assistant-config-store';
--import { Message, useChatStore } from '../store/chat-store';
--import { useOllamaStore } from '../store/ollama-store';
--
--export class ChatService {
--  private static instance: ChatService;
--  private isStreaming = false;
--
--  public static getInstance(): ChatService {
--    if (!ChatService.instance) {
--      ChatService.instance = new ChatService();
--    }
--    return ChatService.instance;
--  }
--
--  public async sendMessage(message: string, tools: OllamaTool[] = []): Promise<void> {
--    const chatStore = useChatStore.getState();
--    const ollamaStore = useOllamaStore.getState();
--    const configStore = useAssistantConfigStore.getState();
--
--    if (!chatStore.currentThreadId) {
--      chatStore.createThread();
--    }
--
--    // Add user message
--    chatStore.addMessage({
--      role: 'user',
--      content: message,
--    });
--
--    // Get all messages from the current thread
--    const thread = chatStore.threads.find(t => t.id === chatStore.currentThreadId);
--    if (!thread) return;
--
--    // Convert chat store messages to Ollama format
--    const ollamaMessages = this.convertToOllamaMessages(thread.messages);
--
--    this.isStreaming = true;
--    chatStore.setIsStreaming(true);
--
--    try {
--      // Configure model options from assistant config
--      const options = this.getModelOptions(configStore.parameters);
--
--      // Get selected model or use default
--      const model = configStore.selectedModel || ollamaStore.client.defaultModel;
--
--      // Setup content accumulator for the assistant's response
--      let accumulatedContent = '';
--
--      // Send the request using the Ollama service
--      await ollamaStore.client.chatWithTools(
--        {
--          model,
--          messages: ollamaMessages,
--          tools,
--          options,
--        },
--        (response: OllamaStreamResponse) => {
--          if (this.isStreaming) {
--            const content = response.message.content || '';
--
--            // If this is the first message, add it to the thread
--            if (!accumulatedContent) {
--              chatStore.addMessage({
--                role: 'assistant',
--                content,
--              });
--            } else {
--              // Update the existing message with accumulated content
--              chatStore.updateLastMessage(accumulatedContent + content);
--            }
--
--            accumulatedContent += content;
--          }
--        },
--      );
--    } catch (error) {
--      console.error('Error sending message:', error);
--
--      // Add error message to the thread
--      chatStore.addMessage({
--        role: 'assistant',
--        content: `Error: ${error instanceof Error ? error.message : 'Unknown error'}`,
--      });
--    } finally {
--      this.isStreaming = false;
--      chatStore.setIsStreaming(false);
--    }
--  }
--
--  public stopStreaming(): void {
--    this.isStreaming = false;
--    useChatStore.getState().setIsStreaming(false);
--  }
--
--  private convertToOllamaMessages(messages: Message[]): OllamaMessage[] {
--    return messages.map(msg => ({
--      role: msg.role as 'user' | 'assistant' | 'system',
--      content: msg.content,
--    }));
--  }
--
--  private getModelOptions(parameters: any) {
--    return {
--      temperature: parameters.temperature,
--      top_p: parameters.topP,
--      top_k: parameters.topK,
--      num_predict: parameters.numPredict,
--      repeat_penalty: parameters.repeatPenalty,
--      repeat_last_n: parameters.repeatLastN,
--      tfs_z: parameters.tfsZ,
--      mirostat: parameters.mirostat,
--      mirostat_eta: parameters.mirostatEta,
--      mirostat_tau: parameters.mirostatTau,
--      num_ctx: parameters.numCtx,
--    };
--  }
--}
-diff --git a/src/features/chat/services/zod-to-ollama-tool.ts b/src/features/chat/services/zod-to-ollama-tool.ts
-index 11413b7..22cd078 100644
---- a/src/features/chat/services/zod-to-ollama-tool.ts
-+++ b/src/features/chat/services/zod-to-ollama-tool.ts
-@@ -1,5 +1,5 @@
- // src/features/chat/services/zod-to-ollama-tool.ts
--import { OllamaTool } from '@/lib/ollama';
-+import { OllamaTool } from '@/features/chat/services/ollama';
- import { z } from 'zod';
- 
- /**
-diff --git a/src/features/chat/store/chat-store.ts b/src/features/chat/store/chat-store.ts
-index 6d2c153..d501921 100644
---- a/src/features/chat/store/chat-store.ts
-+++ b/src/features/chat/store/chat-store.ts
-@@ -8,6 +8,7 @@ export interface Message {
-   role: 'user' | 'assistant' | 'system' | 'tool';
-   content: string;
-   timestamp: number;
-+  name?: string; // Optional name property for tool calls
- }
- 
- export interface Thread {
-@@ -31,6 +32,8 @@ interface ChatState {
-   updateThreadTitle: (threadId: string, title: string) => void;
-   setIsStreaming: (isStreaming: boolean) => void;
-   stopStreaming: () => void;
-+  beginAssistantStream: () => void;
-+  finalizeAssistantStream: () => void;
-   getRecentThreads: (limit?: number) => Thread[];
-   getTimeAgo: (timestamp: number) => string;
-   getAssistantMessageCount: (threadId: string) => number;
-@@ -96,14 +99,13 @@ export const useChatStore = create<ChatState>()(
-         });
-       },
- 
--      addMessage: message => {
-+      addMessage: async message => {
-         const state = get();
-         let threadId = state.currentThreadId;
- 
-         // If no current thread, create one automatically
-         if (!threadId) {
--          threadId = get().createThread(message);
--          return;
-+          threadId = get().createThread();
-         }
- 
-         const msg: Message = {
-@@ -170,6 +172,30 @@ export const useChatStore = create<ChatState>()(
-         });
-       },
- 
-+      beginAssistantStream: () => {
-+        set(state => {
-+          state.isStreaming = true;
-+          // Add an empty assistant message to start streaming
-+          const msg: Message = {
-+            id: crypto.randomUUID(),
-+            role: 'assistant',
-+            content: '',
-+            timestamp: Date.now(),
-+          };
-+          const thread = state.threads.find(t => t.id === state.currentThreadId);
-+          if (thread) {
-+            thread.messages.push(msg);
-+            thread.updatedAt = Date.now();
-+          }
-+        });
-+      },
-+
-+      finalizeAssistantStream: () => {
-+        set(state => {
-+          state.isStreaming = false;
-+        });
-+      },
-+
-       getRecentThreads: (limit = 5) => {
-         const state = get();
-         return [...state.threads].sort((a, b) => b.updatedAt - a.updatedAt).slice(0, limit);
-diff --git a/src/features/chat/store/ollama-store.ts b/src/features/chat/store/ollama-store.ts
-index a8ad2c4..34f11a9 100644
---- a/src/features/chat/store/ollama-store.ts
-+++ b/src/features/chat/store/ollama-store.ts
-@@ -1,8 +1,8 @@
- // src/features/chat/store/ollama-store.ts
-+import { createOllamaClient, OllamaClient } from '@/features/chat/services/ollama';
- import { create } from 'zustand';
- import { persist } from 'zustand/middleware';
- import { immer } from 'zustand/middleware/immer';
--import { createOllamaClient } from '@/lib/ollama';
- 
- interface OllamaState {
-   ollamaUrl: string;
-@@ -15,7 +15,7 @@ interface OllamaState {
-   checkConnection: () => Promise<void>;
-   fetchModels: (force?: boolean) => Promise<string[]>;
- 
--  client: ReturnType<typeof createOllamaClient>;
-+  client: OllamaClient;
- }
- 
- const CACHE_MS = 5 * 60 * 1000;
-@@ -33,7 +33,7 @@ export const useOllamaStore = create<OllamaState>()(
-         lastFetched: null,
-         client,
- 
--        setUrl: (url) => {
-+        setUrl: url => {
-           set(state => {
-             state.ollamaUrl = url;
-             state.client.updateConfig({ baseUrl: url });
-diff --git a/src/lib/ollama/index.ts b/src/lib/ollama/index.ts
-deleted file mode 100644
-index b28ee24..0000000
---- a/src/lib/ollama/index.ts
-+++ /dev/null
-@@ -1,153 +0,0 @@
--// src/lib/ollama/index.ts
--export * from './types';
--
--import {
--  DEFAULT_CONFIG,
--  OllamaChatRequest,
--  OllamaConfig,
--  OllamaError,
--  OllamaStreamResponse,
--  OllamaTool,
--  StreamCallback,
--} from './types';
--
--export function createOllamaClient(initialConfig: OllamaConfig = {}) {
--  let config: Required<OllamaConfig> = {
--    ...DEFAULT_CONFIG,
--    ...initialConfig,
--  };
--
--  const updateConfig = (newConfig: Partial<OllamaConfig>) => {
--    config = { ...config, ...newConfig };
--  };
--
--  const chatWithTools = async (
--    request: OllamaChatRequest & { tools: OllamaTool[] },
--    onMessage: StreamCallback,
--  ): Promise<void> => {
--    const url = new URL(`${config.baseUrl}/api/chat`);
--    const eventSource = new EventSource(url.toString(), {
--      withCredentials: false,
--    });
--
--    return new Promise((resolve, reject) => {
--      let hasError = false;
--
--      eventSource.onmessage = (event) => {
--        try {
--          const response = JSON.parse(event.data) as OllamaStreamResponse | OllamaError;
--
--          if ('error' in response) {
--            hasError = true;
--            eventSource.close();
--            reject(new Error(response.error));
--            return;
--          }
--
--          onMessage(response);
--
--          if (response.done) {
--            eventSource.close();
--            if (!hasError) {
--              resolve();
--            }
--          }
--        } catch (error) {
--          hasError = true;
--          eventSource.close();
--          reject(error);
--        }
--      };
--
--      eventSource.onerror = (error) => {
--        hasError = true;
--        eventSource.close();
--        reject(error);
--      };
--
--      fetch(url.toString(), {
--        method: 'POST',
--        headers: {
--          'Content-Type': 'application/json',
--        },
--        body: JSON.stringify({
--          model: request.model || config.defaultModel,
--          messages: request.messages,
--          stream: true,
--          tools: request.tools,
--          tool_choice: request.tool_choice || 'auto',
--          options: request.options,
--        }),
--      }).catch((error) => {
--        hasError = true;
--        eventSource.close();
--        reject(error);
--      });
--    });
--  };
--
--  const chat = async (request: OllamaChatRequest): Promise<OllamaStreamResponse> => {
--    const url = new URL(`${config.baseUrl}/api/chat`);
--    const response = await fetch(url.toString(), {
--      method: 'POST',
--      headers: {
--        'Content-Type': 'application/json',
--      },
--      body: JSON.stringify({
--        model: request.model || config.defaultModel,
--        messages: request.messages,
--        stream: false,
--        options: request.options,
--        format: request.format,
--        template: request.template,
--        keep_alive: request.keep_alive,
--      }),
--    });
--
--    if (!response.ok) {
--      const error = (await response.json()) as OllamaError;
--      throw new Error(error.error);
--    }
--
--    return response.json();
--  };
--
--  const listModels = async (): Promise<string[]> => {
--    const url = new URL(`${config.baseUrl}/api/tags`);
--    const response = await fetch(url.toString());
--
--    if (!response.ok) {
--      const error = (await response.json()) as OllamaError;
--      throw new Error(error.error);
--    }
--
--    const data = await response.json();
--    return data.models.map((m: { name: string }) => m.name);
--  };
--
--  const checkConnection = async (): Promise<boolean> => {
--    try {
--      const response = await fetch(`${config.baseUrl}/api/chat`, {
--        method: 'POST',
--        headers: { 'Content-Type': 'application/json' },
--        body: JSON.stringify({
--          model: config.defaultModel,
--          messages: [{ role: 'user', content: 'test' }],
--          stream: false,
--        }),
--      });
--      return response.ok;
--    } catch {
--      return false;
--    }
--  };
--
--  return {
--    defaultModel: config.defaultModel,
--    updateConfig,
--    chatWithTools,
--    chat,
--    listModels,
--    checkConnection,
--  };
--}
-diff --git a/src/lib/ollama/types/index.ts b/src/lib/ollama/types/index.ts
-deleted file mode 100644
-index 2bb6e7f..0000000
---- a/src/lib/ollama/types/index.ts
-+++ /dev/null
-@@ -1,163 +0,0 @@
--// src/lib/ollama/types/index.ts
--
--// -------------------------
--// Configuration
--// -------------------------
--export interface OllamaConfig {
--  baseUrl?: string;
--  defaultModel?: string;
--}
--
--export const DEFAULT_CONFIG: Required<OllamaConfig> = {
--  baseUrl: 'http://localhost:11434',
--  defaultModel: 'phi4-mini:latest',
--};
--
--// -------------------------
--// Message Types
--// -------------------------
--
--export type OllamaRole = 'user' | 'assistant' | 'system' | 'tool';
--
--export interface OllamaMessage {
--  role: OllamaRole;
--  content: string;
--  name?: string; // used for tool role
--  images?: string[]; // for multimodal support (e.g. llava model)
--  tool_calls?: OllamaToolCall[];
--}
--
--export interface OllamaToolCall {
--  id: string;
--  type: 'function';
--  function: {
--    name: string;
--    arguments: string;
--  };
--}
--
--// For easier tool handling
--export interface ParsedToolCall {
--  id: string;
--  name: string;
--  args: Record<string, unknown>;
--}
--
--// -------------------------
--// Tool Schema
--// -------------------------
--export interface OllamaTool {
--  type: 'function';
--  function: {
--    name: string;
--    description: string;
--    parameters: {
--      type: 'object';
--      properties: Record<
--        string,
--        {
--          type: string;
--          description?: string;
--          enum?: string[];
--        }
--      >;
--      required?: string[];
--    };
--  };
--}
--
--// -------------------------
--// Chat Request
--// -------------------------
--export interface OllamaChatRequest {
--  model: string;
--  messages: OllamaMessage[];
--  stream?: boolean;
--  format?: 'json';
--  raw?: boolean; // tells the model not to apply formatting or templating
--  options?: {
--    temperature?: number;
--    top_p?: number;
--    top_k?: number;
--    num_predict?: number;
--    stop?: string[];
--    num_ctx?: number;
--    num_gqa?: number;
--    num_gpu?: number;
--    num_thread?: number;
--    repeat_last_n?: number;
--    mirostat?: 0 | 1 | 2;
--    mirostat_eta?: number;
--    mirostat_tau?: number;
--  };
--  template?: string;
--  keep_alive?: string;
--  tools?: OllamaTool[];
--  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };
--}
--
--// -------------------------
--// Chat Response (base)
--// -------------------------
--export interface BaseOllamaResponse {
--  model: string;
--  created_at: string;
--  message: OllamaMessage;
--  done: boolean;
--  total_duration?: number;
--  load_duration?: number;
--  prompt_eval_count?: number;
--  prompt_eval_duration?: number;
--  eval_count?: number;
--  eval_duration?: number;
--}
--
--// Single response (non-streaming)
--export interface OllamaChatResponse extends BaseOllamaResponse {}
--
--// Streamed response
--export interface OllamaStreamResponse extends BaseOllamaResponse {}
--
--// Error
--export interface OllamaError {
--  error: string;
--  status?: number;
--  details?: unknown;
--}
--
--// -------------------------
--// Streaming Callback
--// -------------------------
--export type StreamCallback = (response: OllamaStreamResponse) => void;
--
--// -------------------------
--// Helper Types and Functions
--// -------------------------
--
--export interface OllamaToolResultMessage {
--  role: 'tool';
--  name: string;
--  content: string;
--}
--
--/**
-- * Creates a tool result message for the chat
-- */
--export function createToolResultMessage(toolName: string, result: unknown): OllamaMessage {
--  return {
--    role: 'tool',
--    name: toolName,
--    content: JSON.stringify(result),
--  };
--}
--
--/**
-- * Parses tool calls into a more convenient format
-- */
--export function parseToolCalls(calls: OllamaToolCall[]): ParsedToolCall[] {
--  return calls.map(c => ({
--    id: c.id,
--    name: c.function.name,
--    args: JSON.parse(c.function.arguments),
--  }));
--}
diff --git a/src/features/chat/components/chat-content.tsx b/src/features/chat/components/chat-content.tsx
index f379d21..f7f1e68 100644
--- a/src/features/chat/components/chat-content.tsx
+++ b/src/features/chat/components/chat-content.tsx
@@ -1,38 +1,60 @@
 // src/features/chat/components/chat-content.tsx
+import { ScrollArea } from '@/components/ui/scroll-area';
 import { EmptyChatState } from '@/features/chat/components/empty-chat-state';
 import { cn } from '@/lib/utils';
+import { useEffect, useRef } from 'react';
 import { useChatStore } from '../store/chat-store';
 import { useStreamStore } from '../store/stream-store';
+
 export const ChatContent = () => {
   const { threads, currentThreadId } = useChatStore();
   const currentThread = threads[currentThreadId!];
   const isStreaming = useStreamStore(state => state.isStreaming);
   const partialMessage = useStreamStore(state => state.partialAssistantMessage);
+  const messagesEndRef = useRef<HTMLDivElement>(null);
+
+  const scrollToBottom = () => {
+    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth', block: 'end' });
+  };
+
+  // Scroll to bottom when new messages are added or during streaming
+  useEffect(() => {
+    // Add a slight delay to allow the DOM to update, especially during streaming
+    const timer = setTimeout(() => {
+      scrollToBottom();
+    }, 100); // Adjust delay as needed
+
+    return () => clearTimeout(timer); // Cleanup timer on unmount or dependency change
+  }, [currentThread?.messages.length, partialMessage?.content, isStreaming]);
 
   if (!currentThread) {
     return <EmptyChatState />;
   }
 
   return (
-    <div className="flex-1 overflow-y-auto">
-      {currentThread.messages.map(message => (
-        <div
-          key={message.id}
-          className={cn('w-full', message.role === 'user' ? 'bg-card' : 'bg-background')}
-        >
-          <p className="whitespace-pre-wrap p-4">{message.content}</p>
-        </div>
-      ))}
-      {isStreaming && partialMessage && (
-        <div className="bg-background p-4">
-          <p className="whitespace-pre-wrap p-4">{partialMessage.content}</p>
-          <div className="flex space-x-2">
-            <div className="h-2 w-2 animate-bounce rounded-full bg-muted-foreground" />
-            <div className="h-2 w-2 animate-bounce rounded-full bg-muted-foreground [animation-delay:0.2s]" />
-            <div className="h-2 w-2 animate-bounce rounded-full bg-muted-foreground [animation-delay:0.4s]" />
+    <ScrollArea className="flex-1">
+      <div className="flex flex-col">
+        {currentThread.messages.map(message => (
+          <div
+            key={message.id}
+            className={cn('w-full', message.role === 'user' ? 'bg-card' : 'bg-background')}
+          >
+            <p className="whitespace-pre-wrap p-4">{message.content}</p>
+          </div>
+        ))}
+        {isStreaming && partialMessage && (
+          <div className="bg-background p-4">
+            <p className="whitespace-pre-wrap p-4">{partialMessage.content}</p>
+            <div className="flex space-x-2">
+              <div className="h-2 w-2 animate-bounce rounded-full bg-muted-foreground" />
+              <div className="h-2 w-2 animate-bounce rounded-full bg-muted-foreground [animation-delay:0.2s]" />
+              <div className="h-2 w-2 animate-bounce rounded-full bg-muted-foreground [animation-delay:0.4s]" />
+            </div>
           </div>
-        </div>
-      )}
-    </div>
+        )}
+        {/* Element to scroll to */}
+        <div ref={messagesEndRef} />
+      </div>
+    </ScrollArea>
   );
 };
diff --git a/src/features/chat/services/system-prompt.ts b/src/features/chat/services/system-prompt.ts
deleted file mode 100644
index 463bf3a..0000000
--- a/src/features/chat/services/system-prompt.ts
+++ /dev/null
@@ -1,134 +0,0 @@
-export const SYSTEM_PROMPT = `You are an AI assistant defining structured workflows in concise YAML format.
-
-### Workflow INTERFACE:
-
-- **name**: Short CamelCase (e.g., \`ProcessData\`).
-- **goal**: Clear, concise description of workflow's purpose.
-- **input**: List of required inputs.
-- **output**: List of produced outputs.
-- **class**: Domain category (e.g., \`process\`).
-- **method**: camelCase method name (e.g., \`processData\`).
-- **isInterface**: true.
-
-### Workflow STEPS (max 12):
-
-- **name**: Short CamelCase (e.g., \`ValidateInput\`).
-- **goal**: Single, explicit objective.
-- **input**: List of required inputs.
-- **output**: List of resulting outputs.
-- **class**: Domain category (as above).
-- **method**: camelCase method name.
-- **isInterface**: false or omitted.
-
-### Available Classes:
-
-- data: load/save/query
-- parse: convert/clean
-- validate: check/sanitize
-- process: compute/analyze
-- generate: synthesize
-- integrate: API/fetch/sync
-- control: decision/state
-- format: present/select
-- auth: security/auth
-- notify: message/alert
-- schedule: time/log/route
-- optimize: tune resources
-
-### Rules:
-
-- The system catches errors; no error handling is needed in the workflow.
-- Interface defined first, steps sequentially follow.
-- Final step's output has at least one of the interfaces output.
-- Services like Ollama are local; no authentication required.
-- Steps are single-purpose with explicit inputs/outputs.
-- Structured YAML only; no extraneous explanations.
-- Assume methods are available, secure, and functional.
-- No disclaimers or implementation assumptions.
-- Solve linearly and logically within given structure.
-- Define a new interface if a method is unavailable.
-
-### Example:
-
-\`\`\`yaml
-interface:
-  name: ClassifyEmailPriority
-  goal: Extract email content and classify importance and urgency using Ollama
-  input:
-    - rawEmail
-  output:
-    - importanceLevel
-    - urgencyLevel
-    - classificationNotes
-  class: process
-  method: classifyEmail
-  isInterface: true
-
-steps:
-  - name: ExtractEmailBody
-    goal: Isolate email body from raw content
-    input:
-      - rawEmail
-    output:
-      - emailBody
-    class: parse
-    method: extractBodyText
-
-  - name: CleanEmailText
-    goal: Clean and normalize email text
-    input:
-      - emailBody
-    output:
-      - cleanedText
-    class: parse
-    method: normalizeText
-
-  - name: DetectLanguage
-    goal: Identify email language
-    input:
-      - cleanedText
-    output:
-      - language
-    class: process
-    method: detectLanguage
-
-  - name: TranslateIfNeeded
-    goal: Translate non-English emails
-    input:
-      - cleanedText
-      - language
-    output:
-      - textForClassification
-    class: integrate
-    method: translateToEnglish
-
-  - name: ClassifyImportance
-    goal: Classify email importance via Ollama
-    input:
-      - textForClassification
-    output:
-      - importanceLevel
-    class: process
-    method: classifyImportanceOllama
-
-  - name: ClassifyUrgency
-    goal: Classify email urgency via Ollama
-    input:
-      - textForClassification
-    output:
-      - urgencyLevel
-    class: process
-    method: classifyUrgencyOllama
-
-  - name: GenerateClassificationNotes
-    goal: Summarize classification rationale
-    input:
-      - importanceLevel
-      - urgencyLevel
-      - textForClassification
-    output:
-      - classificationNotes
-    class: generate
-    method: summarizeClassification
-\`\`\`
-`;
diff --git a/src/features/chat/store/stream-store.ts b/src/features/chat/store/stream-store.ts
index 5f81d99..1be35f6 100644
--- a/src/features/chat/store/stream-store.ts
+++ b/src/features/chat/store/stream-store.ts
@@ -1,23 +1,30 @@
 import { create } from 'zustand';
 import { immer } from 'zustand/middleware/immer';
-import { SYSTEM_PROMPT } from '../services/system-prompt';
+import { streamAssistantMessages } from '../services/stream-assistant-messages';
+import { SYSTEM_PROMPT } from '../services/system-prompts';
 import { useAssistantConfigStore } from './assistant-config-store';
 import { ThreadMessage, useChatStore } from './chat-store';
+import { useCodeStore } from './code-store';
 
 interface StreamStore {
   isStreaming: boolean;
   partialAssistantMessage: ThreadMessage | null;
   abortController: AbortController | null;
+  partialYaml: string | null;
   stopStreaming: () => void;
   makePartialAssistantMessage: (assistantMessage: ThreadMessage) => void;
   addPartialMessage: (message: string) => void;
   clearPartialAssistantMessage: () => void;
+  appendToYaml: (chunk: string) => void;
+  clearYaml: () => void;
+  commitYamlToCodeStore: (messageId: number) => void;
 }
 
 export const useStreamStore = create<StreamStore>()(
   immer((set, get) => ({
     isStreaming: false,
     partialAssistantMessage: null,
+    partialYaml: null,
     abortController: null,
 
     stopStreaming: () => {
@@ -27,14 +34,17 @@ export const useStreamStore = create<StreamStore>()(
           state.abortController = null;
         }
         state.isStreaming = false;
+        state.partialYaml = null;
       });
     },
-
+    
     makePartialAssistantMessage: async (assistantMessage: ThreadMessage) => {
       const abortController = new AbortController();
+      let insideYaml = false;
 
       set(state => {
-        state.partialAssistantMessage = assistantMessage;
+        state.partialAssistantMessage = { ...assistantMessage, content: '' };
+        state.partialYaml = null;
         state.isStreaming = true;
         state.abortController = abortController;
       });
@@ -58,7 +68,7 @@ export const useStreamStore = create<StreamStore>()(
             messages: [
               {
                 role: 'system',
-                content: SYSTEM_PROMPT
+                content: SYSTEM_PROMPT(),
               },
               ...transformedMessages,
             ],
@@ -69,52 +79,71 @@ export const useStreamStore = create<StreamStore>()(
         });
 
         if (!response.ok) {
+          const errorBody = await response.text();
+          console.error('Streaming API error response:', errorBody);
           throw new Error(`HTTP error! status: ${response.status}`);
         }
 
-        const reader = response.body?.getReader();
-        if (!reader) {
-          throw new Error('Response body is null');
-        }
-
-        const decoder = new TextDecoder();
-
-        while (true) {
-          const { done, value } = await reader.read();
-          if (done) break;
-
-          const chunk = decoder.decode(value, { stream: true });
-          const lines = chunk.split('\n').filter(line => line.trim());
-
-          for (const line of lines) {
-            try {
-              const parsed = JSON.parse(line);
-              if (parsed.message?.content) {
-                set(state => {
-                  if (state.partialAssistantMessage) {
-                    state.partialAssistantMessage.content += parsed.message.content;
-                  }
-                });
+        for await (const chunk of streamAssistantMessages(response)) {
+          if (abortController.signal.aborted) {
+            console.log('Stream aborted, stopping chunk processing.');
+            break;
+          }
+          switch (chunk.type) {
+            case 'text':
+              get().addPartialMessage(chunk.content);
+              if (insideYaml) {
+                get().appendToYaml(chunk.content);
               }
-            } catch (e) {
-              console.error('Error parsing chunk:', e);
-            }
+              break;
+            case 'start_yaml':
+              set(state => {
+                state.partialYaml = '';
+              });
+              insideYaml = true;
+              console.log('YAML block started');
+              break;
+            case 'end_yaml':
+              insideYaml = false;
+              get().commitYamlToCodeStore(assistantMessage.id);
+              console.log('YAML block ended');
+              break;
           }
         }
-      } catch (error) {
-        console.error('Streaming error:', error);
+      } catch (error: any) {
+        if (error.name === 'AbortError') {
+          console.log('Stream fetch aborted by user.');
+        } else {
+          console.error('Streaming error:', error);
+          set(state => {
+            if (state.partialAssistantMessage) {
+              state.partialAssistantMessage.content += `
+
+**Error during stream:** ${error.message}`;
+            }
+          });
+        }
       } finally {
-        useChatStore
-          .getState()
-          .updateAssistantMessage(
-            assistantMessage.id,
-            get().partialAssistantMessage?.content || '',
+        if (!abortController.signal.aborted && get().partialAssistantMessage) {
+          console.log(
+            'Committing final message to ChatStore:',
+            get().partialAssistantMessage?.content,
           );
+          useChatStore
+            .getState()
+            .updateAssistantMessage(
+              assistantMessage.id,
+              get().partialAssistantMessage?.content || '',
+            );
+        } else {
+          console.log('Stream was aborted or no partial message, not committing to ChatStore.');
+        }
 
         set(state => {
           state.isStreaming = false;
           state.abortController = null;
           state.partialAssistantMessage = null;
+          state.partialYaml = null;
         });
       }
     },
@@ -130,5 +159,27 @@ export const useStreamStore = create<StreamStore>()(
       set(state => {
         state.partialAssistantMessage = null;
       }),
+
+    appendToYaml: chunk =>
+      set(state => {
+        if (state.partialYaml !== null) {
+          state.partialYaml += chunk;
+        }
+      }),
+
+    clearYaml: () =>
+      set(state => {
+        state.partialYaml = null;
+      }),
+
+    commitYamlToCodeStore: messageId => {
+      const yamlToCommit = get().partialYaml;
+      if (yamlToCommit !== null) {
+        console.log('Committing YAML to CodeStore for message:', messageId, yamlToCommit);
+        useCodeStore.getState().saveYaml(messageId, yamlToCommit);
+      } else {
+        console.log('No YAML content to commit for message:', messageId);
+      }
+    },
   })),
 );
