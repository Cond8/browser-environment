diff --git a/output.txt b/output.txt
index fd6678c..e69de29 100644
--- a/output.txt
+++ b/output.txt
@@ -1,408 +0,0 @@
-diff --git a/src/features/chat/components/empty-chat-state.tsx b/src/features/chat/components/empty-chat-state.tsx
-index 74e82ab..8676bc9 100644
---- a/src/features/chat/components/empty-chat-state.tsx
-+++ b/src/features/chat/components/empty-chat-state.tsx
-@@ -24,7 +24,7 @@ export const EmptyChatState: React.FC<EmptyChatStateProps> = ({
-         <div className="mb-4">
-           <MessageSquare className="h-8 w-8 text-muted-foreground" />
-         </div>
--        <h3 className="text-lg font-medium">Welcome to Chat</h3>
-+        <h3 className="text-lg font-medium">Welcome to Cond8 Chat</h3>
-         <p className="text-sm text-muted-foreground mb-6">Start a new conversation or use the shortcuts below</p>
-       </div>
-       <ShortcutsCheatsheet shortcuts={SHORTCUTS} />
-diff --git a/src/features/chat/services/ollama-wrapper.ts b/src/features/chat/services/ollama-wrapper.ts
-deleted file mode 100644
-index 749142b..0000000
---- a/src/features/chat/services/ollama-wrapper.ts
-+++ /dev/null
-@@ -1,84 +0,0 @@
--import { DEFAULT_CONFIG, OllamaConfig } from '../types/ollama-config';
--import { OllamaChatRequest } from '../types/ollama-request';
--import { OllamaStreamResponse, StreamCallback } from '../types/ollama-response';
--
--// Types
--export interface OllamaTool {
--  type: 'function';
--  function: {
--    name: string;
--    description: string;
--    parameters: {
--      type: 'object';
--      properties: Record<
--        string,
--        {
--          type: string;
--          description?: string;
--          enum?: string[];
--        }
--      >;
--      required?: string[];
--    };
--  };
--}
--
--// Utility Functions
--export function createOllamaConfig(config: OllamaConfig = {}): Required<OllamaConfig> {
--  return {
--    ...DEFAULT_CONFIG,
--    ...config,
--  };
--}
--
--export async function chatWithTools(
--  request: OllamaChatRequest & { tools: OllamaTool[] },
--  onMessage: StreamCallback,
--  config: OllamaConfig = {},
--): Promise<void> {
--  const { baseUrl, defaultModel } = createOllamaConfig(config);
--  const url = new URL(`${baseUrl}/api/chat`);
--  const eventSource = new EventSource(url.toString(), {
--    withCredentials: false,
--  });
--
--  return new Promise((resolve, reject) => {
--    eventSource.onmessage = event => {
--      try {
--        const response = JSON.parse(event.data) as OllamaStreamResponse;
--        onMessage(response);
--
--        if (response.done) {
--          eventSource.close();
--          resolve();
--        }
--      } catch (error) {
--        eventSource.close();
--        reject(error);
--      }
--    };
--
--    eventSource.onerror = error => {
--      eventSource.close();
--      reject(error);
--    };
--
--    fetch(url.toString(), {
--      method: 'POST',
--      headers: {
--        'Content-Type': 'application/json',
--      },
--      body: JSON.stringify({
--        model: request.model || defaultModel,
--        messages: request.messages,
--        stream: true,
--        tools: request.tools,
--        tool_choice: request.tool_choice || 'auto',
--        options: request.options,
--      }),
--    }).catch(error => {
--      eventSource.close();
--      reject(error);
--    });
--  });
--}
-diff --git a/src/features/chat/store/chat-store.ts b/src/features/chat/store/chat-store.ts
-index 817a27f..7ba9ad9 100644
---- a/src/features/chat/store/chat-store.ts
-+++ b/src/features/chat/store/chat-store.ts
-@@ -26,7 +26,9 @@ interface ChatState {
-   deleteThread: (threadId: string) => void;
-   setCurrentThread: (threadId: string) => void;
-   addMessage: (message: Omit<Message, 'id' | 'timestamp'>) => void;
-+  updateLastMessage: (content: string) => void;
-   updateThreadTitle: (threadId: string, title: string) => void;
-+  setIsStreaming: (isStreaming: boolean) => void;
-   stopStreaming: () => void;
-   getRecentThreads: (limit?: number) => Thread[];
-   getTimeAgo: (timestamp: number) => string;
-@@ -127,6 +129,24 @@ export const useChatStore = create<ChatState>()(
-         });
-       },
- 
-+      updateLastMessage: content => {
-+        set(state => {
-+          const thread = state.threads.find(t => t.id === state.currentThreadId);
-+          if (thread) {
-+            const lastMessage = thread.messages[thread.messages.length - 1];
-+            if (lastMessage && lastMessage.role === 'assistant') {
-+              // Create a new message object instead of mutating
-+              const updatedMessage = {
-+                ...lastMessage,
-+                content,
-+              };
-+              // Create a new messages array with the updated message
-+              thread.messages = [...thread.messages.slice(0, -1), updatedMessage];
-+            }
-+          }
-+        });
-+      },
-+
-       updateThreadTitle: (threadId, title) => {
-         set(state => {
-           const thread = state.threads.find(t => t.id === threadId);
-@@ -136,6 +156,12 @@ export const useChatStore = create<ChatState>()(
-         });
-       },
- 
-+      setIsStreaming: isStreaming => {
-+        set(state => {
-+          state.isStreaming = isStreaming;
-+        });
-+      },
-+
-       stopStreaming: () => {
-         set(state => {
-           state.isStreaming = false;
-diff --git a/src/features/chat/store/ollama-store.ts b/src/features/chat/store/ollama-store.ts
-index 3f8f496..3a1fc38 100644
---- a/src/features/chat/store/ollama-store.ts
-+++ b/src/features/chat/store/ollama-store.ts
-@@ -1,3 +1,4 @@
-+import { OllamaService } from '@/lib/ollama';
- import { create } from 'zustand';
- import { persist } from 'zustand/middleware';
- import { immer } from 'zustand/middleware/immer';
-@@ -8,6 +9,7 @@ interface OllamaState {
-   isLoading: boolean;
-   error: string | null;
-   lastFetched: number | null;
-+  ollamaService: OllamaService;
- 
-   setUrl: (url: string) => void;
-   checkConnection: () => Promise<void>;
-@@ -24,32 +26,33 @@ export const useOllamaStore = create<OllamaState>()(
-       isLoading: false,
-       error: null,
-       lastFetched: null,
-+      ollamaService: new OllamaService(),
- 
-       setUrl: url => {
-         set(state => {
-           state.ollamaUrl = url;
-+          state.ollamaService.updateConfig({ baseUrl: url });
-         });
-         get().checkConnection();
-       },
- 
-       checkConnection: async () => {
-+        set(state => {
-+          state.isLoading = true;
-+          state.error = null;
-+        });
-+
-         try {
--          const res = await fetch(`${get().ollamaUrl}/api/chat`, {
--            method: 'POST',
--            headers: { 'Content-Type': 'application/json' },
--            body: JSON.stringify({
--              model: 'llama2',
--              messages: [{ role: 'user', content: 'test' }],
--              stream: false,
--            }),
--          });
--          if (!res.ok) throw new Error('Failed to connect to Ollama');
-+          const isConnected = await get().ollamaService.checkConnection();
-+
-           set(state => {
--            state.error = null;
-+            state.error = isConnected ? null : 'Failed to connect to Ollama';
-+            state.isLoading = false;
-           });
-         } catch (err) {
-           set(state => {
-             state.error = err instanceof Error ? err.message : 'Unknown error';
-+            state.isLoading = false;
-           });
-         }
-       },
-@@ -58,12 +61,7 @@ export const useOllamaStore = create<OllamaState>()(
-         const now = Date.now();
-         const state = get();
- 
--        if (
--          !force &&
--          state.models &&
--          state.lastFetched &&
--          now - state.lastFetched < CACHE_MS
--        ) {
-+        if (!force && state.models && state.lastFetched && now - state.lastFetched < CACHE_MS) {
-           return state.models;
-         }
- 
-@@ -73,10 +71,7 @@ export const useOllamaStore = create<OllamaState>()(
-         });
- 
-         try {
--          const res = await fetch(`${state.ollamaUrl}/api/tags`);
--          if (!res.ok) throw new Error('Failed to fetch models');
--          const json = await res.json();
--          const models = json.models.map((m: { name: string }) => m.name);
-+          const models = await state.ollamaService.listModels();
- 
-           set(s => {
-             s.models = models;
-@@ -97,6 +92,6 @@ export const useOllamaStore = create<OllamaState>()(
-     {
-       name: 'ollama-store',
-       partialize: state => ({ ollamaUrl: state.ollamaUrl }),
--    }
--  )
-+    },
-+  ),
- );
-diff --git a/src/features/chat/tools/domain-tool-generator.ts b/src/features/chat/tools/domain-tool-generator.ts
-index 903586b..88b3c7b 100644
---- a/src/features/chat/tools/domain-tool-generator.ts
-+++ b/src/features/chat/tools/domain-tool-generator.ts
-@@ -1,5 +1,5 @@
- import { z } from 'zod';
--import { zodToOllamaTool } from '../services/ollama-wrapper';
-+import { zodToOllamaTool } from '../utils/zod-to-ollama-tool';
- import { Domain, domainClasses, Interface, Step } from './problem-solver';
- 
- // Schema for tool generation
-diff --git a/src/features/chat/tools/problem-solver.ts b/src/features/chat/tools/problem-solver.ts
-index d0a1bf4..126c437 100644
---- a/src/features/chat/tools/problem-solver.ts
-+++ b/src/features/chat/tools/problem-solver.ts
-@@ -1,6 +1,7 @@
-+import { OllamaChatResponse, OllamaService } from '@/lib/ollama';
- import { z } from 'zod';
--import { OllamaChatResponse, OllamaService, zodToOllamaTool } from '../services/ollama-wrapper';
- import { SYSTEM_PROMPT } from '../services/system-prompt';
-+import { zodToOllamaTool } from '../utils/zod-to-ollama-tool';
- 
- // Domain-specific class names with descriptions
- const domainClasses = {
-diff --git a/src/features/chat/types/ollama-config.ts b/src/features/chat/types/ollama-config.ts
-deleted file mode 100644
-index c8802e6..0000000
---- a/src/features/chat/types/ollama-config.ts
-+++ /dev/null
-@@ -1,9 +0,0 @@
--export interface OllamaConfig {
--  baseUrl?: string;
--  defaultModel?: string;
--}
--
--export const DEFAULT_CONFIG: Required<OllamaConfig> = {
--  baseUrl: 'http://localhost:11434',
--  defaultModel: 'phi4-mini:latest',
--};
-diff --git a/src/features/chat/types/ollama-message.ts b/src/features/chat/types/ollama-message.ts
-deleted file mode 100644
-index e40a8a8..0000000
---- a/src/features/chat/types/ollama-message.ts
-+++ /dev/null
-@@ -1,14 +0,0 @@
--export interface OllamaMessage {
--  role: 'user' | 'assistant' | 'system';
--  content: string;
--  tool_calls?: OllamaToolCall[];
--}
--
--export interface OllamaToolCall {
--  id: string;
--  type: 'function';
--  function: {
--    name: string;
--    arguments: string;
--  };
--} 
-\ No newline at end of file
-diff --git a/src/features/chat/types/ollama-request.ts b/src/features/chat/types/ollama-request.ts
-deleted file mode 100644
-index 2630952..0000000
---- a/src/features/chat/types/ollama-request.ts
-+++ /dev/null
-@@ -1,17 +0,0 @@
--import { OllamaMessage } from './ollama-message';
--import { OllamaTool } from './ollama-tool';
--
--export interface OllamaChatRequest {
--  model: string;
--  messages: OllamaMessage[];
--  stream?: boolean;
--  tools?: OllamaTool[];
--  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };
--  options?: {
--    temperature?: number;
--    top_p?: number;
--    top_k?: number;
--    num_predict?: number;
--    stop?: string[];
--  };
--}
-diff --git a/src/features/chat/types/ollama-response.ts b/src/features/chat/types/ollama-response.ts
-deleted file mode 100644
-index f65cb2a..0000000
---- a/src/features/chat/types/ollama-response.ts
-+++ /dev/null
-@@ -1,29 +0,0 @@
--import { OllamaMessage } from './ollama-message';
--
--export interface OllamaChatResponse {
--  model: string;
--  created_at: string;
--  message: OllamaMessage;
--  done: boolean;
--  total_duration?: number;
--  load_duration?: number;
--  prompt_eval_duration?: number;
--  eval_duration?: number;
--}
--
--export interface OllamaStreamResponse {
--  model: string;
--  created_at: string;
--  message: OllamaMessage;
--  done: boolean;
--  total_duration?: number;
--  load_duration?: number;
--  prompt_eval_duration?: number;
--  eval_duration?: number;
--}
--
--export interface OllamaError {
--  error: string;
--}
--
--export type StreamCallback = (response: OllamaStreamResponse) => void; 
-\ No newline at end of file
-diff --git a/src/features/chat/types/ollama-tool.ts b/src/features/chat/types/ollama-tool.ts
-deleted file mode 100644
-index 96d07a6..0000000
---- a/src/features/chat/types/ollama-tool.ts
-+++ /dev/null
-@@ -1,19 +0,0 @@
--export interface OllamaTool {
--  type: 'function';
--  function: {
--    name: string;
--    description: string;
--    parameters: {
--      type: 'object';
--      properties: Record<
--        string,
--        {
--          type: string;
--          description?: string;
--          enum?: string[];
--        }
--      >;
--      required?: string[];
--    };
--  };
--} 
-\ No newline at end of file
-diff --git a/src/features/chat/utils/zod-to-ollama-tool.ts b/src/features/chat/utils/zod-to-ollama-tool.ts
-index dfee448..0a390c6 100644
---- a/src/features/chat/utils/zod-to-ollama-tool.ts
-+++ b/src/features/chat/utils/zod-to-ollama-tool.ts
-@@ -1,5 +1,5 @@
-+import { OllamaTool } from '@/lib/ollama';
- import { z } from 'zod';
--import { OllamaTool } from '../services/ollama-wrapper';
- 
- export function zodToOllamaTool(
-   schema: z.ZodType<any>,
diff --git a/src/features/chat/components/_chat-runner.tsx b/src/features/chat/components/_chat-runner.tsx
index 70a0c28..4bd6ae6 100644
--- a/src/features/chat/components/_chat-runner.tsx
+++ b/src/features/chat/components/_chat-runner.tsx
@@ -1,114 +1,14 @@
 // src/features/chat/components/_chat-runner.tsx
-import { useEffect, useRef } from 'react';
+import { useEffect } from 'react';
 import { useChatStore } from '../store/chat-store';
-import { useOllamaStore } from '../store/ollama-store';
-import { useAssistantConfigStore } from '../store/assistant-config-store';
-import { allTools } from '../tools';
+import { runAssistantStream } from '../lib/stream-runner';
 
 export function ChatRunner() {
-  const {
-    isStreaming,
-    setIsStreaming,
-    currentThreadId,
-    threads,
-    updateLastMessage,
-    addMessage,
-  } = useChatStore();
-  const { selectedModel, parameters } = useAssistantConfigStore();
-  const { client } = useOllamaStore();
-
-  const isRunningRef = useRef(false);
+  const { isStreaming, currentThreadId } = useChatStore();
 
   useEffect(() => {
-    if (!isStreaming || isRunningRef.current || !currentThreadId) return;
-
-    const thread = threads.find(t => t.id === currentThreadId);
-    if (!thread) return;
-
-    const messages = thread.messages.map(m => ({
-      role: m.role,
-      content: m.content,
-    }));
-
-    let assistantContent = '';
-    isRunningRef.current = true;
-    addMessage({ role: 'assistant', content: '' });
-
-    client.chatWithTools(
-      {
-        model: selectedModel ?? 'default',
-        messages,
-        tools: allTools.map(t => t.tool),
-        tool_choice: 'auto',
-        options: parameters,
-      },
-      async (chunk) => {
-        // Handle streamed content
-        if ('content' in chunk && typeof chunk.content === 'string') {
-          assistantContent += chunk.content;
-          updateLastMessage(assistantContent);
-        }
-
-        // ðŸ§  Tool call support
-        if (chunk.message.tool_calls) {
-          for (const call of chunk.message.tool_calls) {
-            const tool = allTools.find(t => t.tool.function.name === call.function.name);
-            if (!tool) continue;
-
-            try {
-              const parsedArgs = JSON.parse(call.function.arguments || '{}');
-              const result = tool.parser.parse(parsedArgs); // validate with Zod
-
-              // Add the tool call to the thread
-              addMessage({
-                role: 'tool',
-                content: JSON.stringify(result, null, 2),
-              });
-
-              // Resume the conversation with tool result
-              const continuation = await client.chat({
-                model: selectedModel ?? 'default',
-                messages: [
-                  ...messages,
-                  {
-                    role: 'tool',
-                    name: call.function.name,
-                    content: JSON.stringify(result),
-                  },
-                ],
-                options: parameters,
-              });
-
-              if ('content' in continuation.message) {
-                addMessage({
-                  role: 'assistant',
-                  content: continuation.message.content,
-                });
-              }
-            } catch (err) {
-              console.error('Tool call error', err);
-              addMessage({
-                role: 'system',
-                content: `Tool execution failed: ${err instanceof Error ? err.message : 'Unknown error'}`,
-              });
-            }
-
-            setIsStreaming(false);
-            isRunningRef.current = false;
-            return;
-          }
-        }
-
-        if (chunk.done) {
-          setIsStreaming(false);
-          isRunningRef.current = false;
-        }
-      },
-    ).catch((err) => {
-      console.error('Streaming failed:', err);
-      setIsStreaming(false);
-      isRunningRef.current = false;
-    });
+    if (!isStreaming || !currentThreadId) return;
+    void runAssistantStream(currentThreadId);
   }, [isStreaming, currentThreadId]);
 
   return null;
diff --git a/src/features/chat/components/chat-box.tsx b/src/features/chat/components/chat-box.tsx
deleted file mode 100644
index a0c09f4..0000000
--- a/src/features/chat/components/chat-box.tsx
+++ /dev/null
@@ -1,85 +0,0 @@
-// src/features/chat/components/chat-box.tsx
-import React, { useState } from 'react';
-import { ChatService } from '../services/chat-service';
-import { useChatStore } from '../store/chat-store';
-import { useAssistantConfigStore } from '../store/assistant-config-store';
-import { useOllamaStore } from '../store/ollama-store';
-
-export function ChatBox() {
-  const [input, setInput] = useState('');
-  const { threads, currentThreadId, isStreaming } = useChatStore();
-  const { ollamaUrl } = useOllamaStore();
-  const { selectedModel } = useAssistantConfigStore();
-  
-  const chatService = ChatService.getInstance();
-  
-  const currentThread = threads.find(t => t.id === currentThreadId);
-  const messages = currentThread?.messages || [];
-
-  const handleSendMessage = async (e: React.FormEvent) => {
-    e.preventDefault();
-    
-    if (!input.trim() || isStreaming) return;
-    
-    const message = input.trim();
-    setInput('');
-    
-    await chatService.sendMessage(message);
-  };
-
-  const handleStopGeneration = () => {
-    chatService.stopStreaming();
-  };
-
-  return (
-    <div className="flex flex-col h-full">
-      <div className="p-2 text-sm text-gray-600 bg-gray-100 border-b">
-        Connected to: {ollamaUrl} | Model: {selectedModel || 'Default'}
-      </div>
-      
-      <div className="flex-1 overflow-auto p-4 space-y-4">
-        {messages.map(message => (
-          <div 
-            key={message.id} 
-            className={`${
-              message.role === 'user' ? 'bg-blue-100' : 'bg-gray-100'
-            } p-3 rounded-lg max-w-md ${
-              message.role === 'user' ? 'ml-auto' : 'mr-auto'
-            }`}
-          >
-            <div className="text-xs text-gray-500 mb-1">
-              {message.role === 'user' ? 'You' : 'Assistant'}
-            </div>
-            <div className="whitespace-pre-wrap">{message.content}</div>
-          </div>
-        ))}
-      </div>
-      
-      <form onSubmit={handleSendMessage} className="p-4 border-t flex">
-        <input
-          value={input}
-          onChange={e => setInput(e.target.value)}
-          placeholder="Type a message..."
-          className="flex-1 p-2 border rounded-l-md focus:outline-none focus:ring-2 focus:ring-blue-400"
-          disabled={isStreaming}
-        />
-        {isStreaming ? (
-          <button
-            type="button"
-            onClick={handleStopGeneration}
-            className="px-4 py-2 bg-red-500 text-white rounded-r-md hover:bg-red-600"
-          >
-            Stop
-          </button>
-        ) : (
-          <button
-            type="submit"
-            className="px-4 py-2 bg-blue-500 text-white rounded-r-md hover:bg-blue-600"
-          >
-            Send
-          </button>
-        )}
-      </form>
-    </div>
-  );
-} 
\ No newline at end of file
diff --git a/src/features/chat/services/chat-service.ts b/src/features/chat/services/chat-service.ts
deleted file mode 100644
index 3b37dc6..0000000
--- a/src/features/chat/services/chat-service.ts
+++ /dev/null
@@ -1,121 +0,0 @@
-// src/features/chat/services/chat-service.ts
-import { OllamaMessage, OllamaStreamResponse, OllamaTool } from '@/lib/ollama';
-import { useAssistantConfigStore } from '../store/assistant-config-store';
-import { Message, useChatStore } from '../store/chat-store';
-import { useOllamaStore } from '../store/ollama-store';
-
-export class ChatService {
-  private static instance: ChatService;
-  private isStreaming = false;
-
-  public static getInstance(): ChatService {
-    if (!ChatService.instance) {
-      ChatService.instance = new ChatService();
-    }
-    return ChatService.instance;
-  }
-
-  public async sendMessage(message: string, tools: OllamaTool[] = []): Promise<void> {
-    const chatStore = useChatStore.getState();
-    const ollamaStore = useOllamaStore.getState();
-    const configStore = useAssistantConfigStore.getState();
-
-    if (!chatStore.currentThreadId) {
-      chatStore.createThread();
-    }
-
-    // Add user message
-    chatStore.addMessage({
-      role: 'user',
-      content: message,
-    });
-
-    // Get all messages from the current thread
-    const thread = chatStore.threads.find(t => t.id === chatStore.currentThreadId);
-    if (!thread) return;
-
-    // Convert chat store messages to Ollama format
-    const ollamaMessages = this.convertToOllamaMessages(thread.messages);
-
-    this.isStreaming = true;
-    chatStore.setIsStreaming(true);
-
-    try {
-      // Configure model options from assistant config
-      const options = this.getModelOptions(configStore.parameters);
-
-      // Get selected model or use default
-      const model = configStore.selectedModel || ollamaStore.client.defaultModel;
-
-      // Setup content accumulator for the assistant's response
-      let accumulatedContent = '';
-
-      // Send the request using the Ollama service
-      await ollamaStore.client.chatWithTools(
-        {
-          model,
-          messages: ollamaMessages,
-          tools,
-          options,
-        },
-        (response: OllamaStreamResponse) => {
-          if (this.isStreaming) {
-            const content = response.message.content || '';
-
-            // If this is the first message, add it to the thread
-            if (!accumulatedContent) {
-              chatStore.addMessage({
-                role: 'assistant',
-                content,
-              });
-            } else {
-              // Update the existing message with accumulated content
-              chatStore.updateLastMessage(accumulatedContent + content);
-            }
-
-            accumulatedContent += content;
-          }
-        },
-      );
-    } catch (error) {
-      console.error('Error sending message:', error);
-
-      // Add error message to the thread
-      chatStore.addMessage({
-        role: 'assistant',
-        content: `Error: ${error instanceof Error ? error.message : 'Unknown error'}`,
-      });
-    } finally {
-      this.isStreaming = false;
-      chatStore.setIsStreaming(false);
-    }
-  }
-
-  public stopStreaming(): void {
-    this.isStreaming = false;
-    useChatStore.getState().setIsStreaming(false);
-  }
-
-  private convertToOllamaMessages(messages: Message[]): OllamaMessage[] {
-    return messages.map(msg => ({
-      role: msg.role as 'user' | 'assistant' | 'system',
-      content: msg.content,
-    }));
-  }
-
-  private getModelOptions(parameters: any) {
-    return {
-      temperature: parameters.temperature,
-      top_p: parameters.topP,
-      top_k: parameters.topK,
-      num_predict: parameters.numPredict,
-      repeat_penalty: parameters.repeatPenalty,
-      repeat_last_n: parameters.repeatLastN,
-      tfs_z: parameters.tfsZ,
-      mirostat: parameters.mirostat,
-      mirostat_eta: parameters.mirostatEta,
-      mirostat_tau: parameters.mirostatTau,
-      num_ctx: parameters.numCtx,
-    };
-  }
-}
diff --git a/src/features/chat/services/zod-to-ollama-tool.ts b/src/features/chat/services/zod-to-ollama-tool.ts
index 11413b7..22cd078 100644
--- a/src/features/chat/services/zod-to-ollama-tool.ts
+++ b/src/features/chat/services/zod-to-ollama-tool.ts
@@ -1,5 +1,5 @@
 // src/features/chat/services/zod-to-ollama-tool.ts
-import { OllamaTool } from '@/lib/ollama';
+import { OllamaTool } from '@/features/chat/services/ollama';
 import { z } from 'zod';
 
 /**
diff --git a/src/features/chat/store/chat-store.ts b/src/features/chat/store/chat-store.ts
index 6d2c153..d501921 100644
--- a/src/features/chat/store/chat-store.ts
+++ b/src/features/chat/store/chat-store.ts
@@ -8,6 +8,7 @@ export interface Message {
   role: 'user' | 'assistant' | 'system' | 'tool';
   content: string;
   timestamp: number;
+  name?: string; // Optional name property for tool calls
 }
 
 export interface Thread {
@@ -31,6 +32,8 @@ interface ChatState {
   updateThreadTitle: (threadId: string, title: string) => void;
   setIsStreaming: (isStreaming: boolean) => void;
   stopStreaming: () => void;
+  beginAssistantStream: () => void;
+  finalizeAssistantStream: () => void;
   getRecentThreads: (limit?: number) => Thread[];
   getTimeAgo: (timestamp: number) => string;
   getAssistantMessageCount: (threadId: string) => number;
@@ -96,14 +99,13 @@ export const useChatStore = create<ChatState>()(
         });
       },
 
-      addMessage: message => {
+      addMessage: async message => {
         const state = get();
         let threadId = state.currentThreadId;
 
         // If no current thread, create one automatically
         if (!threadId) {
-          threadId = get().createThread(message);
-          return;
+          threadId = get().createThread();
         }
 
         const msg: Message = {
@@ -170,6 +172,30 @@ export const useChatStore = create<ChatState>()(
         });
       },
 
+      beginAssistantStream: () => {
+        set(state => {
+          state.isStreaming = true;
+          // Add an empty assistant message to start streaming
+          const msg: Message = {
+            id: crypto.randomUUID(),
+            role: 'assistant',
+            content: '',
+            timestamp: Date.now(),
+          };
+          const thread = state.threads.find(t => t.id === state.currentThreadId);
+          if (thread) {
+            thread.messages.push(msg);
+            thread.updatedAt = Date.now();
+          }
+        });
+      },
+
+      finalizeAssistantStream: () => {
+        set(state => {
+          state.isStreaming = false;
+        });
+      },
+
       getRecentThreads: (limit = 5) => {
         const state = get();
         return [...state.threads].sort((a, b) => b.updatedAt - a.updatedAt).slice(0, limit);
diff --git a/src/features/chat/store/ollama-store.ts b/src/features/chat/store/ollama-store.ts
index a8ad2c4..34f11a9 100644
--- a/src/features/chat/store/ollama-store.ts
+++ b/src/features/chat/store/ollama-store.ts
@@ -1,8 +1,8 @@
 // src/features/chat/store/ollama-store.ts
+import { createOllamaClient, OllamaClient } from '@/features/chat/services/ollama';
 import { create } from 'zustand';
 import { persist } from 'zustand/middleware';
 import { immer } from 'zustand/middleware/immer';
-import { createOllamaClient } from '@/lib/ollama';
 
 interface OllamaState {
   ollamaUrl: string;
@@ -15,7 +15,7 @@ interface OllamaState {
   checkConnection: () => Promise<void>;
   fetchModels: (force?: boolean) => Promise<string[]>;
 
-  client: ReturnType<typeof createOllamaClient>;
+  client: OllamaClient;
 }
 
 const CACHE_MS = 5 * 60 * 1000;
@@ -33,7 +33,7 @@ export const useOllamaStore = create<OllamaState>()(
         lastFetched: null,
         client,
 
-        setUrl: (url) => {
+        setUrl: url => {
           set(state => {
             state.ollamaUrl = url;
             state.client.updateConfig({ baseUrl: url });
diff --git a/src/lib/ollama/index.ts b/src/lib/ollama/index.ts
deleted file mode 100644
index b28ee24..0000000
--- a/src/lib/ollama/index.ts
+++ /dev/null
@@ -1,153 +0,0 @@
-// src/lib/ollama/index.ts
-export * from './types';
-
-import {
-  DEFAULT_CONFIG,
-  OllamaChatRequest,
-  OllamaConfig,
-  OllamaError,
-  OllamaStreamResponse,
-  OllamaTool,
-  StreamCallback,
-} from './types';
-
-export function createOllamaClient(initialConfig: OllamaConfig = {}) {
-  let config: Required<OllamaConfig> = {
-    ...DEFAULT_CONFIG,
-    ...initialConfig,
-  };
-
-  const updateConfig = (newConfig: Partial<OllamaConfig>) => {
-    config = { ...config, ...newConfig };
-  };
-
-  const chatWithTools = async (
-    request: OllamaChatRequest & { tools: OllamaTool[] },
-    onMessage: StreamCallback,
-  ): Promise<void> => {
-    const url = new URL(`${config.baseUrl}/api/chat`);
-    const eventSource = new EventSource(url.toString(), {
-      withCredentials: false,
-    });
-
-    return new Promise((resolve, reject) => {
-      let hasError = false;
-
-      eventSource.onmessage = (event) => {
-        try {
-          const response = JSON.parse(event.data) as OllamaStreamResponse | OllamaError;
-
-          if ('error' in response) {
-            hasError = true;
-            eventSource.close();
-            reject(new Error(response.error));
-            return;
-          }
-
-          onMessage(response);
-
-          if (response.done) {
-            eventSource.close();
-            if (!hasError) {
-              resolve();
-            }
-          }
-        } catch (error) {
-          hasError = true;
-          eventSource.close();
-          reject(error);
-        }
-      };
-
-      eventSource.onerror = (error) => {
-        hasError = true;
-        eventSource.close();
-        reject(error);
-      };
-
-      fetch(url.toString(), {
-        method: 'POST',
-        headers: {
-          'Content-Type': 'application/json',
-        },
-        body: JSON.stringify({
-          model: request.model || config.defaultModel,
-          messages: request.messages,
-          stream: true,
-          tools: request.tools,
-          tool_choice: request.tool_choice || 'auto',
-          options: request.options,
-        }),
-      }).catch((error) => {
-        hasError = true;
-        eventSource.close();
-        reject(error);
-      });
-    });
-  };
-
-  const chat = async (request: OllamaChatRequest): Promise<OllamaStreamResponse> => {
-    const url = new URL(`${config.baseUrl}/api/chat`);
-    const response = await fetch(url.toString(), {
-      method: 'POST',
-      headers: {
-        'Content-Type': 'application/json',
-      },
-      body: JSON.stringify({
-        model: request.model || config.defaultModel,
-        messages: request.messages,
-        stream: false,
-        options: request.options,
-        format: request.format,
-        template: request.template,
-        keep_alive: request.keep_alive,
-      }),
-    });
-
-    if (!response.ok) {
-      const error = (await response.json()) as OllamaError;
-      throw new Error(error.error);
-    }
-
-    return response.json();
-  };
-
-  const listModels = async (): Promise<string[]> => {
-    const url = new URL(`${config.baseUrl}/api/tags`);
-    const response = await fetch(url.toString());
-
-    if (!response.ok) {
-      const error = (await response.json()) as OllamaError;
-      throw new Error(error.error);
-    }
-
-    const data = await response.json();
-    return data.models.map((m: { name: string }) => m.name);
-  };
-
-  const checkConnection = async (): Promise<boolean> => {
-    try {
-      const response = await fetch(`${config.baseUrl}/api/chat`, {
-        method: 'POST',
-        headers: { 'Content-Type': 'application/json' },
-        body: JSON.stringify({
-          model: config.defaultModel,
-          messages: [{ role: 'user', content: 'test' }],
-          stream: false,
-        }),
-      });
-      return response.ok;
-    } catch {
-      return false;
-    }
-  };
-
-  return {
-    defaultModel: config.defaultModel,
-    updateConfig,
-    chatWithTools,
-    chat,
-    listModels,
-    checkConnection,
-  };
-}
diff --git a/src/lib/ollama/types/index.ts b/src/lib/ollama/types/index.ts
deleted file mode 100644
index 2bb6e7f..0000000
--- a/src/lib/ollama/types/index.ts
+++ /dev/null
@@ -1,163 +0,0 @@
-// src/lib/ollama/types/index.ts
-
-// -------------------------
-// Configuration
-// -------------------------
-export interface OllamaConfig {
-  baseUrl?: string;
-  defaultModel?: string;
-}
-
-export const DEFAULT_CONFIG: Required<OllamaConfig> = {
-  baseUrl: 'http://localhost:11434',
-  defaultModel: 'phi4-mini:latest',
-};
-
-// -------------------------
-// Message Types
-// -------------------------
-
-export type OllamaRole = 'user' | 'assistant' | 'system' | 'tool';
-
-export interface OllamaMessage {
-  role: OllamaRole;
-  content: string;
-  name?: string; // used for tool role
-  images?: string[]; // for multimodal support (e.g. llava model)
-  tool_calls?: OllamaToolCall[];
-}
-
-export interface OllamaToolCall {
-  id: string;
-  type: 'function';
-  function: {
-    name: string;
-    arguments: string;
-  };
-}
-
-// For easier tool handling
-export interface ParsedToolCall {
-  id: string;
-  name: string;
-  args: Record<string, unknown>;
-}
-
-// -------------------------
-// Tool Schema
-// -------------------------
-export interface OllamaTool {
-  type: 'function';
-  function: {
-    name: string;
-    description: string;
-    parameters: {
-      type: 'object';
-      properties: Record<
-        string,
-        {
-          type: string;
-          description?: string;
-          enum?: string[];
-        }
-      >;
-      required?: string[];
-    };
-  };
-}
-
-// -------------------------
-// Chat Request
-// -------------------------
-export interface OllamaChatRequest {
-  model: string;
-  messages: OllamaMessage[];
-  stream?: boolean;
-  format?: 'json';
-  raw?: boolean; // tells the model not to apply formatting or templating
-  options?: {
-    temperature?: number;
-    top_p?: number;
-    top_k?: number;
-    num_predict?: number;
-    stop?: string[];
-    num_ctx?: number;
-    num_gqa?: number;
-    num_gpu?: number;
-    num_thread?: number;
-    repeat_last_n?: number;
-    mirostat?: 0 | 1 | 2;
-    mirostat_eta?: number;
-    mirostat_tau?: number;
-  };
-  template?: string;
-  keep_alive?: string;
-  tools?: OllamaTool[];
-  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };
-}
-
-// -------------------------
-// Chat Response (base)
-// -------------------------
-export interface BaseOllamaResponse {
-  model: string;
-  created_at: string;
-  message: OllamaMessage;
-  done: boolean;
-  total_duration?: number;
-  load_duration?: number;
-  prompt_eval_count?: number;
-  prompt_eval_duration?: number;
-  eval_count?: number;
-  eval_duration?: number;
-}
-
-// Single response (non-streaming)
-export interface OllamaChatResponse extends BaseOllamaResponse {}
-
-// Streamed response
-export interface OllamaStreamResponse extends BaseOllamaResponse {}
-
-// Error
-export interface OllamaError {
-  error: string;
-  status?: number;
-  details?: unknown;
-}
-
-// -------------------------
-// Streaming Callback
-// -------------------------
-export type StreamCallback = (response: OllamaStreamResponse) => void;
-
-// -------------------------
-// Helper Types and Functions
-// -------------------------
-
-export interface OllamaToolResultMessage {
-  role: 'tool';
-  name: string;
-  content: string;
-}
-
-/**
- * Creates a tool result message for the chat
- */
-export function createToolResultMessage(toolName: string, result: unknown): OllamaMessage {
-  return {
-    role: 'tool',
-    name: toolName,
-    content: JSON.stringify(result),
-  };
-}
-
-/**
- * Parses tool calls into a more convenient format
- */
-export function parseToolCalls(calls: OllamaToolCall[]): ParsedToolCall[] {
-  return calls.map(c => ({
-    id: c.id,
-    name: c.function.name,
-    args: JSON.parse(c.function.arguments),
-  }));
-}
